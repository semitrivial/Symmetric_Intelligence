\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\def\LH{\textrm{LH}}
\def\SYM{S}

\title{Symmetric Complexity: On Legg-Hutter Universal Intelligence with Punishments}
\author{Samuel Allen Alexander}

\begin{document}

\maketitle

\begin{abstract}
    Can an agent's intelligence level be negative?
    We extend the Legg-Hutter agent-environment framework to include punishments
    and argue for an affirmative answer to that question.
    We show that if the background Universal Turing Machine (UTM) admits
    vertically symmetric Kolmogorov complexities,
    then the resulting Legg-Hutter intelligence measure is symmetric about
    the origin (in particular, this implies reward-ignoring agents
    have Legg-Hutter intelligence $0$ according to such UTMs). This
    symmetry property is one answer to a question of Leike and Hutter,
    ``But what are other desirable properties of a UTM?'' Having provided one
    answer to that question, we also address the question of natural UTMs
    more generally, vindicating Legg-Hutter intelligence.
\end{abstract}

\section{Introduction}

In their paper \cite{legg2007universal}, Legg and Hutter write:
\begin{quote}
    ``As our goal is to produce a definition of intelligence that is as broad and
    encompassing as possible, the space of environments used in our definition should
    be as large as possible.''
\end{quote}
So motivated, we investigate what would happen if we extended the universe
of environments to include environments with rewards from $\mathbb Q\cap [-1,1]$
instead of just from $\mathbb Q\cap [0,1]$ as in Legg and Hutter's paper.
In other words, we investigate what would happen if environments are not only
allowed to reward agents but also to punish agents (a punishment being a negative
reward).

We discovered that when negative rewards are allowed, this
introduces a certain algebraic structure into the agent-environment framework. The
main objection we anticipate to our extended framework
is that it implies the negative intelligence of certain
agents\footnote{Thus, this paper falls under the broader
program of advocating for intelligence measures having different ranges than
the nonnegative reals. Alexander has advocated
more extreme extensions of the range of intelligence measures
\cite{alexander2020archimedean} \cite{alexander2021measuring}; by contrast,
here we merely question the
assumption that intelligence never be negative, leaving aside the
question of whether intelligence should be real-valued.}.
We would argue that this makes perfect sense when environments are capable of punishing
agents: the intelligence of a reinforcement learning agent should measure the
degree to which that agent can extract large rewards on average across many environments;
an agent who instead extracts large punishments on average across many environments
should therefore have a negative intelligence level.

Leike and Hutter \cite{leike2015bad} pointed out that Legg-Hutter intelligence
(and related agents such as AIXI \cite{hutter2004universal}) are sensitive
to the background choice of a Universal
Turing Machine (UTM). Certain choices of UTM can make AIXI and Legg-Hutter intelligence
apparently perform badly. Leike and Hutter lamented the lack of satisfactory answers to
the question of how to choose a ``natural'' UTM. This paper is more optimistic:
we show that certain UTM qualities improve the performance of Legg-Hutter
intelligence. We will argue (in Remark \ref{leikeresponse} below)
that Legg-Hutter intelligence's UTM-sensitivity is
a feature, not a bug, and opine that the quest for ``the one true UTM'' is
a red herring.

The structure of the paper is as follows:
\begin{itemize}
    \item In Section \ref{prelimsecn}, we give preliminary definitions.
    \item In Section \ref{dualsection}, we introduce what we call the dual of an agent and of
        an environment, and prove some algebraic theorems about these.
    \item In Section \ref{mainsecn}, we show the existence of UTMs yielding Kolmogorov
        complexities with certain symmetries, and show that the resulting Legg-Hutter
        intelligence measures are symmetric too.
    \item In Section \ref{conclusionsecn}, we summarize and make concluding remarks, including
        remarks about how these ideas might be applied to certain other intelligence measures.
\end{itemize}

\section{Preliminaries}
\label{prelimsecn}

In defining agent and environment below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify which well-behaved
environments to restrict our attention to).

Throughout the paper, we implicitly
fix a finite set $\mathcal A$ of \emph{actions},
a finite set $\mathcal O$ of \emph{observations},
and a finite set $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards}
(so each reward is a rational number between $-1$ and $1$ inclusive),
with $|\mathcal A|>1$,
$|\mathcal O|>0$, $|\mathcal R|>0$.
We assume that $\mathcal R$ has the following property:
whenever $\mathcal R$ contains any reward $r$, then $\mathcal R$
also contains $-r$.
We assume $\mathcal A$, $\mathcal O$, and $\mathcal R$ are mutually disjoint
(i.e., no reward is an action, no reward is an observation, and no action is an
observation).
By $\langle\rangle$ we mean the empty sequence.

\begin{definition}
\label{omnibusdefn}
    (Agents, environments, etc.)
    \begin{enumerate}
        \item
        By $(\mathcal O\mathcal R\mathcal A)^*$ we mean the set of
        all finite sequences starting with an observation, ending with an action,
        and following the pattern ``observation, reward, action, ...''.
        We include $\langle\rangle$ in this set.
        \item
        By $(\mathcal O\mathcal R\mathcal A)^* \mathcal O\mathcal R$
        we mean the set of all sequences of the form $s\frown o\frown r$ where
        $s\in (\mathcal O\mathcal R\mathcal A)^*$, $o\in\mathcal O$
        and $r\in\mathcal R$ ($\frown$ denotes concatenation).
        \item
        By an \emph{agent}, we mean a function $\pi$ which assigns to every sequence
        $s\in (\mathcal O\mathcal R\mathcal A)^* \mathcal O\mathcal R$ a $\mathbb Q$-valued probability measure,
        written $\pi(\bullet|s)$, on $\mathcal A$.
        For every such $s$ and every $a\in\mathcal A$,
        we write $\pi(a|s)$ for $(\pi(\bullet|s))(a)$.
        Intuitively, $\pi(a|s)$ is the probability that agent $\pi$
        will take action $a$ in response to history $s$.
        \item
        By an \emph{environment}, we mean a function $\mu$
        which assigns to every
        $s\in (\mathcal O\mathcal R\mathcal A)^*$
        a $\mathbb Q$-valued probability measure,
        written $\mu(\bullet|s)$,
        on $\mathcal O\times\mathcal R$.
        For every such $s$ and every $(o,r)\in\mathcal O\times\mathcal R$,
        we write $\mu(o,r|s)$ for $(\mu(\bullet|s))(o,r)$.
        Intuitively, $\mu(o,r|s)$ is the probability that environment
        $\mu$ will issue observation $o$ and reward $r$ to the agent in response
        to history $s$.
        \item
        If $\pi$ is an agent, $\mu$ is an environment, and $n\in\mathbb N$,
        we write $V^\pi_{\mu,n}$ for the expected value of the sum of
        the first $n$ rewards which would occur in the sequence
        $(o_0,r_0,a_0,\ldots,o_n,r_n,a_n)$ randomly generated as follows:
        \begin{enumerate}
            \item $(o_0,r_0)\in \mathcal O\times\mathcal R$ is chosen randomly based
            on the probability measure $\mu(\bullet|\langle\rangle)$.
            \item $a_0\in\mathcal A$ is chosen randomly based on the probability
            measure $\pi(\bullet|o_0,r_0)$.
            \item
            For each $i>0$,
            $(o_i,r_i)\in\mathcal O\times\mathcal R$ is chosen randomly based on
            the probability measure
            $\mu(\bullet|o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1})$.
            \item
            For each $i>0$,
            $a_i\in\mathcal A$ is chosen randomly based on the probability measure
            $\pi(\bullet|o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1},o_i,r_i)$.
        \end{enumerate}
        \item
        If $\pi$ is an agent and $\mu$ is an environment,
        let $V^\pi_\mu=\lim_{n\to\infty}V^{\pi}_{\mu,n}$.
        Intuitively, $V^\pi_\mu$ is the expected total reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

Note that it is possible for $V^\pi_\mu$ to be undefined.
For example, if $\mu$ is an environment which always issues
reward $(-1)^n$ in response to the agent's $n$th action,
then $V^\pi_\mu$ is undefined for every agent $\pi$.
This would not be the case if rewards were required to be $\geq 0$,
so this is one way in which allowing
punishments complicates the resulting theory.

In order to define Legg-Hutter-style
intelligence measures, it is, in any case, necessary to restrict attention to
certain well-behaved environments. Legg and Hutter (in their Section 3.2)
restrict attention to environments $\mu$ that never give total reward more than $1$,
from which it follows that $V^\pi_\mu\leq 1$. This implication is less clear
when environments can punish the agent, so we take a different approach:
rather than limit the total reward the environment can give and use that to
force $V^\pi_\mu$ to be bounded, we will cut the middleman and
directly assume that $V^\pi_\mu$ is bounded.

\begin{definition}
    An environment $\mu$ is \emph{well-behaved} if $\mu$ is computable and the following
    condition holds: for every agent $\pi$, $V^\pi_\mu$ exists and
    $-1\leq V^\pi_\mu\leq 1$.
\end{definition}

Note that reward-space $[0,1]$ can be transformed into punishment-space
$[-1,0]$ either by means of $r\mapsto -r$ or by means of $r\mapsto r-1$.
An advantage of $r\mapsto -r$ is that it preserves well-behavedness of
environments (we will prove this below in Corollary \ref{wisminuswcorollary}),
whereas $r\mapsto r-1$ does not. For example, an environment which always
issues reward $0$ (so is well-behaved)
would become an environment which always issues reward $-1$ (so is ill-behaved).
The authors of an interesting paper on reinforcement learning suicide/death
\cite{martin2016death} indicate surprise that certain agents change their behavior
when the reward-space $[0,1]$ is transformed into the punishment-space $[-1,0]$,
because said agents do not normally change their behavior in response to positive
linear reward transformations like $r\mapsto r-1$; said behavior-changes are
less surprising when one views the reward transformation as being the result of
$r\mapsto -r$ instead of $r\mapsto r-1$.

\section{Dual Agents and Dual Environments}
\label{dualsection}

In the Introduction, we promised that by allowing environments to punish agents,
we would reveal algebraic structure not otherwise present. The key to this additional
structure is the following definition.

\begin{definition}
(Dual Agents and Dual Environments)
\begin{enumerate}
    \item
    For any sequence $s$, let $\overline s$ be the sequence obtained
    by replacing every reward $r$ in $s$ by $-r$.
    \item
    Suppose $\pi$ is an agent.
    We define a new agent $\overline \pi$, the \emph{dual} of $\pi$,
    as follows:
    for each $s\in (\mathcal O\mathcal R\mathcal A)^*\mathcal O\mathcal R$,
    for each action $a\in\mathcal A$,
    \[\overline\pi(a|s)=\pi(a|\overline s).\]
    \item
    Suppose $\mu$ is an environment.
    We define a new environment $\overline\mu$, the \emph{dual} of $\mu$,
    as follows:
    for each $s\in (\mathcal O\mathcal R\mathcal A)^*$,
    for each observation $o\in\mathcal O$
    and reward $r\in\mathcal R$,
    \[\overline\mu(o,r|s)=\mu(o,-r|\overline s).\]
\end{enumerate}
\end{definition}

\begin{lemma}
\label{doublesubtractionlemma}
(Double Negation)
    \begin{enumerate}
        \item For each sequence $s$, $\overline{\overline s}=s$.
        \item For each agent $\pi$, $\overline{\overline \pi}=\pi$.
        \item For each environment $\mu$, $\overline{\overline \mu}=\mu$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Follows from the fact that for every real number $r$, $--r=r$.
\end{proof}

\begin{theorem}
\label{bigtheorem}
    Suppose $\mu$ is an environment and $\pi$ is an agent.
    Then
    \[
        V^{\overline \pi}_{\overline \mu}=-V^\pi_\mu
    \]
    (and the left-hand side is defined if and only if the right-hand side is defined).
\end{theorem}

\begin{proof}
    By Definition \ref{omnibusdefn} part 6,
    it suffices to show that for each $n\in\mathbb N$,
    $V^{\overline \pi}_{\overline \mu,n}=-V^\pi_{\mu,n}$.
    For that, it suffices to show that for every
    $s\in ((\mathcal O\mathcal R\mathcal A)^*)
    \cup ((\mathcal O\mathcal R\mathcal A)^*\mathcal O\mathcal R)$,
    the probability $X$ of generating $s$ using $\pi$ and $\mu$
    (as in Definition \ref{omnibusdefn} part 5)
    equals the probability $X'$ of generating $\overline s$
    using $\overline \pi$ and $\overline \mu$.
    We will show this by induction on the length of $s$.

    Case 1: $s$ is empty. Then $X=X'=1$.

    Case 2: $s$ terminates with a reward.
    Then $s=t\frown o \frown r$
    for some $t\in (\mathcal O\mathcal R\mathcal A)^*$.
    Let $Y$ (resp.\ $Y'$) be the probability of generating $t$
    (resp.\ $\overline t$)
    using $\pi$ and $\mu$ (resp.\ $\overline \pi$ and $\overline \mu$). We reason:
    \begin{align*}
        X &= \mu(o,r|t)Y
            &\mbox{(Definition of $X$)}\\
          &= \mu(o,--r| \overline{\overline t})Y
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
          &= \overline \mu(o,-r| \overline t)Y
            &\mbox{(Definition of $\overline\mu$)}\\
          &= \overline \mu(o,-r| \overline t)Y'
            &\mbox{(By induction, $Y=Y'$)}\\
          &= X'. &\mbox{(Definition of $X'$)}
    \end{align*}

    Case 3: $s$ terminates with an action.
    Then $s=t\frown a$ for some $t\in(\mathcal O\mathcal R\mathcal A)^*\mathcal O\mathcal R$.
    Let $Y$ (resp.\ $Y'$) be the probability of generating $t$
    (resp.\ $\overline t$)
    using $\pi$ and $\mu$ (resp.\ $\overline\pi$ and $\overline\mu$). We reason:
    \begin{align*}
        X &= \pi(a|t)Y
            &\mbox{(Definition of $X$)}\\
          &= \pi(a|\overline{\overline t})Y
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
          &= \overline \pi(a|\overline t)Y
            &\mbox{(Definition of $\overline \pi$)}\\
          &= \overline \pi(a|\overline t)Y'
            &\mbox{(By induction, $Y=Y'$)}\\
          &= X'. &\mbox{(Definition of $X'$)}
    \end{align*}
\end{proof}

\begin{corollary}
\label{twistcorollary}
    For every agent $\pi$ and environment $\mu$,
    \[V^\pi_{\overline\mu}=-V^{\overline\pi}_{\mu}\]
    (and the left-hand side is defined if and only if the right-hand side is defined).
\end{corollary}

\begin{proof}
    If neither side is defined, then there is nothing to prove.
    Assume the left-hand side is defined. Then
    \begin{align*}
        V^\pi_{\overline\mu}
        &= V^{\overline{\overline\pi}}_{\overline\mu}
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
        &= -V^{\overline\pi}_\mu, &\mbox{(Theorem \ref{bigtheorem})}
    \end{align*}
    as desired. A similar argument holds if we assume the right-hand side is defined.
\end{proof}

\begin{corollary}
\label{wisminuswcorollary}
    For every environment $\mu$, $\mu$ is well-behaved if and only if $\overline\mu$
    is well-behaved.
\end{corollary}

\begin{proof}
    We prove the $\Rightarrow$ direction, the other is similar.
    Since $\mu$ is well-behaved, $\mu$ is computable, so clearly $\overline\mu$ is computable.
    Let $\pi$ be any agent. Since $\mu$ is well-behaved, $V^{\overline\pi}_\mu$ is defined
    and $-1\leq V^{\overline\pi}_\mu\leq 1$.
    By Corollary \ref{twistcorollary},
    $V^\pi_{\overline\mu}=-V^{\overline\pi}_\mu$ is defined,
    implying $-1\leq V^\pi_{\overline\mu}\leq 1$.
    By arbitrariness of $\pi$, this shows $\overline\mu$ is well-behaved.
\end{proof}

\section{Symmetric Intelligence}
\label{mainsecn}

By definition, agent $\overline\pi$ acts exactly as agent $\pi$ would
act if $\pi$ were confused into thinking that punishments were rewards
and rewards were punishments.
Whatever ingenuity $\pi$ applies to maximize rewards,
$\overline\pi$ applies that exact same ingenuity to maximize punishments.
Thus, if $\Gamma$ is an intelligence function (intended to measure how
well an agent extracts rewards in some aggregate sense across the whole
infinite space of all well-behaved environments), then $\Gamma$ ought
to satisfy the equation:
\[
    \Gamma(\overline\pi) = -\Gamma(\pi).
\]
Not every intelligence function satisfies the above equation, but the
above equation should be considered desirable when we
invent ways of measuring intelligence: all else being equal, an
intelligence function which satisfies the above equation should be
considered better than an intelligence function which does
not\footnote{This depends strongly on our assumption that when we
measure intelligence we are measuring aggregate reward-extraction. This might clash
with everyday ideas of intelligence: if a mischievous child answers every single
question wrong on a true-false IQ test, we would say that child is clearly very
intelligent, and is using said intelligence to deliberately fail the test.
To quote Socrates, ``Don't you think the ignorant person would often involuntarily
tell the truth when he wished to say falsehoods, if it so happened, because he
didn't know; whereas you, the wise person, if you should wish to lie,
would always consistently lie?''\ \cite{lesserhippias}
Likewise if an agent consistently manages to extract the worst possible rewards
from many environments, we might think the agent is brilliant but
masochistic.
Nevertheless, we would not want to put said agent
in charge of navigating environments for us (at least not without some
reverse psychology).}.
In this section, we will show that the universal intelligence measure
proposed by Legg and Hutter satisfies the above equation, provided the
background UTM is suitably chosen.

We write $2^*$ for the set of finite binary strings.
We write $f:\subseteq A\to B$ to indicate that $f$ has codomain $B$
and that $f$'s domain is some subset of $A$.

\begin{definition}
    (Prefix-free universal Turing machines)
    \begin{enumerate}
        \item A partial computable function $f:\subseteq 2^*\to 2^*$
        is \emph{prefix-free} if the following requirement holds:
        $\forall p,p'\in 2^*$, if $p$ is a strict initial segment of $p'$,
        then $f(p)$ and $f(p')$ are not both defined.
        \item A \emph{prefix-free universal Turing machine}
        (or \emph{PFUTM}) is a prefix-free
        partial computable function $U:\subseteq 2^*\to 2^*$
        such that the following condition holds.
        For every prefix-free partial computable function
        $f:\subseteq 2^*\to 2^*$, $\exists y\in 2^*$ such that
        $\forall x\in 2^*$, $f(x)=U(y\frown x)$.
        In this case, we say $y$ is a \emph{computer program for
        $f$ in programming language $U$}.
    \end{enumerate}
\end{definition}

Fix a computable prefix-free, postfix-free binary encoding of
$(\mathcal O \mathcal R\mathcal A)^*$, in other words,
fix a computable function
$\ulcorner\bullet\urcorner:(\mathcal O \mathcal R\mathcal A)^*\to 2^*$
assigning a distinct code $\ulcorner s\urcorner\in 2^*$ to every sequence
$s\in (\mathcal O \mathcal R\mathcal A)^*$ in such a way that
$\ulcorner s\urcorner$ is never an initial segment nor terminal segment
of $\ulcorner s'\urcorner$
for any distinct $s,s'\in (\mathcal O \mathcal R\mathcal A)^*$.
That this can be done is clear by the Church-Turing thesis.
Likewise, computably fix codes $\ulcorner m\urcorner$ for all
$\mathbb Q$-valued probability-measures $m$ on $\mathcal O\times\mathcal R$.

\begin{definition}
(Kolmogorov Complexity)
Suppose $U$ is a PFUTM.
\begin{enumerate}
    \item
    For each computable environment $\mu$, the \emph{Kolmogorov complexity of $\mu$
    given by $U$}, written $K_U(\mu)$, is the smallest $n\in\mathbb N$ such that
    there is some computer program of length $n$, in programming language $U$,
    for the function
    $f$
    defined by
    $f(\ulcorner s\urcorner)=\ulcorner \mu(\bullet|s)\urcorner$
    for all $s\in (\mathcal O \mathcal R\mathcal A)^*$.
    \item
    We say $U$ is \emph{vertically symmetric in its reinforcement-learning
    environment
    cross-section} (or simply that $U$ is \emph{vertically symmetric}) if
    $K_U(\mu)=K_U(\overline\mu)$ for every computable environment $\mu$.
\end{enumerate}
\end{definition}

Strictly speaking, $K_U(\mu)$ implicitly depends on our encodings
$\ulcorner \bullet\urcorner$, but we suppress this in the notation because
including it would not add insight.

\begin{theorem}
\label{envirosymmetricexistencelemma}
    There exists a vertically symmetric PFUTM.
\end{theorem}

\begin{proof}
    Let $U_0$ be a PFUTM, we will modify $U_0$ to obtain a vertically symmetric PFUTM.
    For readability's sake, write $\mathrm{POS}$ for $0$ and $\mathrm{NEG}$ for $1$.
    Thinking of $U_0$ as a programming language, we define a new programming language
    $U$ as follows. Every program in $U$ must begin with one of the keywords
    $\mathrm{POS}$ or $\mathrm{NEG}$. Outputs of $U$ are defined as follows.
    \begin{itemize}
        \item $U(\mathrm{POS}\frown x)=U_0(x)$.
        \item To compute $U(\mathrm{NEG}\frown x)$, find
        $s\in (\mathcal O \mathcal R\mathcal A)^*$ such that
        $x=y\frown \ulcorner s\urcorner$ for some $y$ (if no such $s$ exists, diverge).
        Note that $s$ is unique by postfix-freeness of $\ulcorner\bullet\urcorner$.
        If $
            U_0(y\frown
            \ulcorner \hspace{.2em} \overline s\hspace{.2em}\urcorner)
            =\ulcorner m\urcorner
        $
        for some $\mathbb Q$-valued probability-measure $m$ on
        $\mathcal O\times\mathcal R$, then let
        $U(\mathrm{NEG}\frown x)=\ulcorner\hspace{.2em} \overline m\hspace{.2em}\urcorner$
        where $\overline m(o,r)=m(o,-r)$.
        Otherwise, diverge.
        \begin{itemize}
            \item Informally:
            If $x$ appears to be an instruction to plug $s$ into computer
            program $y$ to get a probability measure $\mu(\bullet|s)$, then
            instead plug $\overline s$ into $y$ and flip the resulting
            probability measure so that the output
            ends up being the flipped version of $\mu(\bullet|\overline s)$,
            i.e., $\overline \mu(\bullet|s)$.
        \end{itemize}
    \end{itemize}
    By construction, whenever $\mathrm{POS}\frown y$ is a $U$-computer program
    for $\ulcorner s\urcorner\mapsto \ulcorner \mu(\bullet|s)\urcorner$
    (loosely speaking, a computer program for $\mu$),
    $\mathrm{NEG}\frown y$ is an equal-length $U$-computer program
    for $\ulcorner s\urcorner\mapsto \ulcorner \overline\mu(\bullet|s)\urcorner$
    (loosely speaking, a computer program for $\overline \mu$),
    and vice versa.
    It follows that $U$ is vertically symmetric.
\end{proof}

The proof of Theorem \ref{envirosymmetricexistencelemma} proves more than required:
every PFUTM can be modified to make a vertically symmetric PFUTM. In some sense,
the construction in the proof of Theorem \ref{envirosymmetricexistencelemma} works
by eliminating bias: reinforcement learning itself is implicitly biased in its
convention that rewards be positive and punishments negative. We can imagine
a pessimistic parallel universe\footnote{Perhaps inspired by \cite{foucault2012discipline}.}
where RL instead follows the opposite convention, and the
RL in that parallel universe is no less valid than the RL in our own. To be
unbiased in this sense, a computer program defining an environment
should specify which of the two RL regimes it is operating in (hence the
$\mathrm{POS}$ and $\mathrm{NEG}$ keywords).

\begin{definition}
    Let $W$ be the set of all well-behaved environments.
    Let $\overline W=\{\overline\mu\,:\,\mu\in W\}$.
\end{definition}

\begin{lemma}
\label{WequalsminusWlemma}
$W=\overline W$.
\end{lemma}

\begin{proof}
    By Corollary \ref{wisminuswcorollary}.
\end{proof}

\begin{definition}
\label{universalintelligencedefn}
For every agent $\pi$
and PFUTM $U$, the \emph{Legg-Hutter universal intelligence of $\pi$ given
by $U$}, written $\LH_U(\pi)$, is
\[
    \LH_U(\pi) = \sum_{\mu \in W} 2^{-K_U(\mu)}V^\pi_\mu.
\]
\end{definition}

The sum defining $\LH_U(\pi)$ is absolutely convergent because the summands
in absolute value are dominated by the summands defining Chaitin's constant
(this is why we insist on the UTM being prefix-free).
Thus, by a well-known
theorem from elementary calculus, the sum does not depend on which order the $\mu\in W$
are enumerated.

The following theorem shows that for every vertically symmetric PFUTM $U$, $\LH_U$
satisfies the above desideratum.

\begin{theorem}
\label{maintheorem}
(Symmetry about the origin)
    For every agent $\pi$ and vertically symmetric PFUTM $U$,
    \[
        \LH_U(\overline\pi) = -\LH_U(\pi).
    \]
\end{theorem}

\begin{proof}
    Compute:
    \begin{align*}
        \LH_U(\overline\pi) &= \sum_{\mu\in W} 2^{-K_U(\mu)}V^{\overline\pi}_\mu
            &\mbox{(Definition \ref{universalintelligencedefn})}\\
          &= -\sum_{\mu\in W} 2^{-K_U(\mu)}V^\pi_{\overline\mu}
            &\mbox{(Corollary \ref{twistcorollary})}\\
          &= -\sum_{\mu\in W} 2^{-K_U(\overline\mu)}V^\pi_{\overline\mu}
            &\mbox{($U$ is vertically symmetric)}\\
          &= -\sum_{\mu\in \overline W} 2^{-K_U(\mu)}V^\pi_\mu
            &\mbox{(Change of variables)}\\
          &= -\sum_{\mu\in W} 2^{-K_U(\mu)}V^\pi_\mu
            &\mbox{(Lemma \ref{WequalsminusWlemma})}\\
          &= -\LH_{U}(\pi).
            &\mbox{(Definition \ref{universalintelligencedefn})}
    \end{align*}
\end{proof}

The above desideratum, that $\Gamma(\overline\pi)=-\Gamma(\pi)$, applies to
numerical intelligence measures. If one is merely interested in binary
intelligence comparators (such as those in \cite{alexander2019intelligence}),
the desideratum can be weakened into a non-numerical comparator form:
    If $\pi$ is more intelligent than $\rho$,
    then $\overline\pi$ should be less intelligent than $\overline\rho$.
The following corollary addresses this desideratum.

\begin{corollary}
\label{comparatorcorollary}
    Let $U$ be a vertically symmetric PFUTM.
    For any agents $\pi$ and $\rho$, if $\LH_U(\pi)>\LH_U(\rho)$
    then $\LH_U(\overline\pi)<\LH_U(\overline\rho)$.
\end{corollary}

\begin{proof}
    By Theorem \ref{maintheorem} and basic algebra.
\end{proof}

The following corollary shows that with suitable choice of UTM,
Legg-Hutter universal intelligence satisfies another obvious desideratum.

\begin{corollary}
\label{ignoringrewardscorollary}
    Let $U$ be a vertically symmetric PFUTM and
    suppose $\pi$ is an agent which ignores rewards (by which we mean that
    $\pi(\bullet|s)$ does not depend on the rewards in $s$).
    Then $\LH_U(\pi)=0$.
\end{corollary}

\begin{proof}
    The hypothesis clearly implies $\pi=\overline\pi$. Thus by Theorem
    \ref{maintheorem}, $\LH_U(\pi)=-\LH_U(\pi)$,
    forcing $\LH_U(\pi)=0$.
\end{proof}

Corollary \ref{ignoringrewardscorollary} illustrates why it is appropriate, for
purposes of Legg-Hutter universal intelligence, to choose a vertically symmetric PFUTM.
Consider an agent $\pi_a$
which always blindly repeats a fixed action $a\in\mathcal A$.
For any particular environment $\mu$,
where $\pi_a$ earns some total reward $r$ by blind luck,
that total reward ought to be cancelled out by $\overline\mu$, where
the exact same blind luck becomes blind misfortune and $\pi_a$ earns total reward
$-r$ (Corollary \ref{twistcorollary}). But if $K_U(\mu)\not=K_U(\overline\mu)$,
then the different weights $2^{-K_U(\mu)}\not=2^{-K_U(\overline\mu)}$ would prevent
these two outcomes from cancelling each other.

\begin{remark}
\label{leikeresponse}
\emph{(Responding to Leike and Hutter \cite{leike2015bad})
The above paragraph suggests that in the context of Legg-Hutter universal intelligence,
vertical symmetry is one answer to Leike and Hutter's question,
``But what are other desirable properties of a UTM?'' We would argue it also sheds light
on the more general question of what makes a UTM ``natural''. Vertical symmetry is
natural for reinforcement learning but would be irrelevant elsewhere---just
as adding first-class webprogramming support to C would be inappropriate for
non-webprogramming applications.
We opine that in the same way, this might explain the failure of
attempts \cite{muller2010stationary} to find UTMs that are ``natural'' across-the-board.
We would even go as far as to suggest that for
Legg-Hutter universal intelligence, UTM-sensitivity is a feature, not a bug. There are many
kinds of intelligence, and UTM-sensitivity
reflects this. Some programming
languages are better for some contexts, and for any given context, Legg-Hutter intelligence
would work best with a UTM appropriate for that context. One could even use $\LH_U$ to
measure chess intelligence, by contriving the programming language $U$ so that chess-related
environments have shorter source-codes and chess-irrelevant environments have longer
source-codes!}
\end{remark}

We conclude this section with an exercise, suggesting how the techniques of this paper
can be used to obtain other structural results.

\begin{exercise} (Permutations)
    \begin{enumerate}
        \item
        For each permutation $P:\mathcal A\to\mathcal A$ of the action-space,
        for each sequence $s$,
        let $Ps$ be the result of applying $P$ to all the actions in $s$.
        For each agent $\pi$, let $P\pi$ be the agent defined by
        $P\pi(a|s)=\pi(Pa|Ps)$. For each environment $\mu$, let
        $P\mu$ be the environment defined by
        $P\mu(o,r|s)=\mu(o,r|Ps)$. Show that in general
        $V^\pi_\mu = V^{P\pi}_{P^{-1}\mu}$
        and
        $V^{P\pi}_\mu = V^\pi_{P\mu}$.
        \item
        Say that a PFUTM $U$ is \emph{permutable} if
        $K_U(\mu)=K_U(P\mu)$ for every computable environment $\mu$
        and permutation $P:\mathcal A\to\mathcal A$. Imitating the proof of
        Theorem \ref{envirosymmetricexistencelemma}, show how any given
        PFUTM can be transformed into a permutable PFUTM.
        \item
        Show that if $U$ is a permutable PFUTM, then $\LH_U(P\pi)=\LH_U(\pi)$
        for every agent $\pi$ and permutation $P:\mathcal A\to\mathcal A$.
        \item
        Modify this exercise to apply to permutations
        of the observation-space.
    \end{enumerate}
\end{exercise}

\section{Conclusion}
\label{conclusionsecn}

By allowing environments to punish agents,
we found additional algebraic structure in the agent-environment
framework. Using this, we showed
that Kolmogorov complexity symmetries lead to
Legg-Hutter intelligence symmetries.

In future work it would be interesting to explore how these symmetries
manifest themselves in other Legg-Hutter-like intelligence measures
\cite{gavane} \cite{hernandez}.
% such as the anytime
% intelligence measure of Hern{\'a}ndez-Orallo and Dowe \cite{hernandez}
% and the realtime intelligence measure of Gavane \cite{gavane}.
The precise strategy we employ in this
paper is not relevant to Hibbardian sequence-prediction intelligence
\cite{hibbard} \cite{alexander2021measuring}. However, a higher-level
idea still applies:
a predictor who intentionally mis-predicts
should be considered less intelligent than a $0$-intelligence blind guesser.

A high-level takeaway of
this paper is that if intelligence is supposed to measure how much
reward an agent extracts in aggregate across many environments,
including environments capable of punishment, then such
intelligence measures should sometimes output negative
values.


\bibliographystyle{plain}
\bibliography{sym}
\end{document}
