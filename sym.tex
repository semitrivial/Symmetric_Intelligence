\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\def\LH{\textrm{LH}}
\def\SYM{\textrm{SYM}}

\title{Symmetric Intelligence: An Improvement to Legg-Hutter Universal Intelligence}
\author{Samuel Allen Alexander}

\begin{document}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In their brilliant paper \cite{legg2007universal}, Legg and Hutter write:
\begin{quote}
    ``As our goal is to produce a definition of intelligence that is as broad and
    encompassing as possible, the space of environments used in our definition should
    be as large as possible.''
\end{quote}
So motivated, we investigated what would happen if we extended the universe
of environments to include environments with rewards from $\mathbb Q\cap [-1,1]$
instead of just from $\mathbb Q\cap [0,1]$ as in Legg and Hutter's paper.
In other words, we investigated what would happen if environments are not only
allowed to reward agents but also to punish agents (a punishment being a negative
reward).

We discovered that when negative rewards are allowed, this
introduces a certain algebraic structure into the agent-environment framework, which
in turn allows an intelligence definition that is both better-behaved and also
easier to compute. The main objection we anticipate to our proposed improved
intelligence measure is that it assigns negative intelligence to certain agents.
We will argue that this makes sense when environments are capable of punishing
agents: the intelligence of a reinforcement learning agent should measure the
degree to which that agent can extract large rewards on average across many environments;
an agent who instead extracts large punishments on average across many environments
should therefore have a negative intelligence level.

\section{Preliminaries}

In defining agent and environment below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify which well-behaved
environments to restrict our attention to).

Throughout the paper, we implicitly
fix a finite set $\mathcal A$ of \emph{actions},
a finite set $\mathcal O$ of \emph{observations},
and a finite set $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards}
(so each reward is a rational number between $-1$ and $1$ inclusive),
with $|\mathcal A|>1$,
$|\mathcal O|>0$, $|\mathcal R|>0$.
We assume that $\mathcal R$ satisfies the following property:
whenever $\mathcal R$ contains any reward $r$, then $\mathcal R$
also contains $-r$.
We assume $\mathcal A$, $\mathcal O$, and $\mathcal R$ are mutually disjoint
(i.e., no reward is an action, no reward is an observation, and no action is an
observation).
The acronym ORA stands for ``observation-reward-action''.
Throughout the paper, $\langle\rangle$ denotes the empty sequence.

\begin{definition}
\label{omnibusdefn}
    (Agents and environments)
    \begin{enumerate}
        \item
        By an \emph{ORA-sequence}, we mean a sequence $s$ such that the following requirements
        hold for every natural number $n$:
        \begin{itemize}
            \item If $s$ has a $(3n)$th element, that element is an observation.
            \item If $s$ has a $(3n+1)$th element, that element is a reward.
            \item If $s$ has a $(3n+2)$th element, that element is an action.
        \end{itemize}
        Thus, informally speaking, a non-empty ORA-sequence
        looks like ``observation, reward, action, observation, reward, action, ...'';
        if it terminates (i.e., if it is finite), it may terminate with
        either an observation, a reward, or an action.
        \item
        By an \emph{ORA-prompt}, we mean a non-empty ORA-sequence which
        terminates with a reward.
        \item
        By an \emph{ORA-play}, we mean an ORA-sequence which is either empty or else
        terminates with an action.
        \item
        By an \emph{agent}, we mean a function $\pi$ which assigns to every
        ORA-prompt $p$ a probability measure $\pi(p)$ on $\mathcal A$.
        For each $a\in\mathcal A$,
        we write $\pi(a|p)$ for the probability assigned to $a$ by $\pi(p)$.
        Intuitively, we think of $\pi(a|p)$ as the probability that the agent $\pi$
        will take action $a$ in response to the ORA-prompt $p$.
        \item
        By an \emph{environment}, we mean a function $\mu$
        which assigns to every ORA-play $p$ a probability measure $\mu(p)$
        on $\mathcal O\times\mathcal R$.
        For each $o\in\mathcal O$ and $r\in\mathcal R$, we write $\mu(\langle o,r\rangle|p)$
        for the probability assigned to $(o,r)$ by $\mu(p)$.
        Intuitively, we think of $\mu(\langle o,r\rangle|p)$ as the
        probability that the environment
        $\mu$ will issue observation $o$ and reward $r$ to the agent in response
        to the ORA-play $p$.
        \item
        If $\pi$ is an agent, $\mu$ is an environment, and $n\in\mathbb N$,
        we write $V^\pi_{\mu,n}$ for the expected value of the sum of
        the first $n$ rewards which would occur in an ORA-sequence
        $(o_0,r_0,a_0,\ldots,o_n,r_n)$ randomly generated as follows:
        \begin{enumerate}
            \item $(o_0,r_0)\in \mathcal O\times\mathcal R$ is chosen randomly based
            on the probability measure $\mu(\langle\rangle)$.
            \item $a_0\in\mathcal A$ is chosen randomly based on the probability
            measure $\pi(o_0,r_0)$.
            \item
            For each $i>0$,
            $(o_i,r_i)\in\mathcal O\times\mathcal R$ is chosen randomly based on
            the probability measure $\mu(o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1})$.
            \item
            For each $i>0$,
            $a_i\in\mathcal A$ is chosen randomly based on the probability measure
            $\pi(o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1},o_i,r_i)$.
        \end{enumerate}
        \item
        If $\pi$ is an agent and $\mu$ is an environment,
        let $V^\pi_\mu=\lim_{n\to\infty}V^{\pi}_{\mu,n}$.
        Intuitively, $V^\pi_\mu$ is the expected reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

Note that it is possible for $V^\pi_\mu$ to be undefined.
For example, if $\mu$ is an environment which always issues
reward $(-1)^n$ in response to the agent's $n$th action,
then $V^\pi_\mu$ is undefined for every agent $\pi$.
This would not be the case if rewards were required to be positive,
so this is one way in which allowing
punishments complicates the resulting theory.

In order to define Legg-Hutter-style
intelligence measures, it is, in any case, necessary to restrict attention to
certain well-behaved environments. Legg and Hutter (in their Section 3.2)
restrict attention to environments $\mu$ that never give total reward more than $1$,
from which it follows that $V^\pi_\mu\leq 1$. This implication is less clear
when environments can punish the agent, so we take a different approach:
rather than limit the total reward the environment can give and use that to
force $V^\pi_\mu$ to be well-behaved, we will cut the middleman and
directly assume that $V^\pi_\mu$ is well-behaved.

\begin{definition}
    An environment $\mu$ is \emph{well-behaved} if the following
    condition holds: for every agent $\pi$, $V^\pi_\mu$ exists and
    $-1\leq V^\pi_\mu\leq 1$.
\end{definition}

With the machinery defined so far, it would be possible to now define an
intelligence measure in the same way as Legg and Hutter,
declaring the intelligence of an agent $\pi$ to be
$\sum_{\mu} 2^{-K(\mu)}V^\pi_\mu$ where $\mu$ ranges over the universe
of all well-behaved environments and $K(\mu)$ is the Kolmogorov complexity
of $\mu$. But we will instead propose a different intelligence measure
which apparently has better structural properties. First, though, we need
to introduce (in its debut appearance) some additional notions.

\section{Dual Agents and Dual Environments}

In the Introduction, we promised that by allowing environments to punish agents,
we would reveal algebraic structure which is not present when environments are
not allowed to punish agents. The key to this additional structure is the following
definition.

\begin{definition}
(Dual Agents and Dual Environments)
\begin{enumerate}
    \item
    Suppose $s$ is an ORA-sequence. Let $-s$
    be the ORA-sequence which is equal to $s$ in every way except that,
    whenever the $(3n+1)$th element of $s$ is a reward $r$,
    then the $(3n+1)$th element of $-s$ is $-r$.
    \item
    Suppose $\pi$ is an agent.
    We define a new agent $-\pi$, the \emph{dual} of $\pi$,
    as follows:
    for each ORA-prompt $p$, for each action $a\in\mathcal A$,
    \[(-\pi)(a|p)=\pi(a|{-p}).\]
    \item
    Suppose $\mu$ is an environment.
    We define a new environment $-\mu$, the \emph{dual} of $\mu$,
    as follows:
    for each ORA-play $p$, for each observation $o\in\mathcal O$
    and reward $r\in\mathcal R$,
    \[(-\mu)(\langle o,r\rangle|p)=\mu(\langle o,-r\rangle|{-p}).\]
\end{enumerate}
\end{definition}

\begin{lemma}
\label{doublesubtractionlemma}
(Double Negation)
    \begin{enumerate}
        \item For each ORA-sequence $s$, $--s=s$.
        \item For each agent $\pi$, $--\pi=\pi$.
        \item For each environment $\mu$, $--\mu=\mu$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Follows from the fact that for every real number $r$, $--r=r$.
\end{proof}

In the proof of the following theorem, we write $\frown$ for concatenation.

\begin{theorem}
\label{bigtheorem}
    Suppose $\mu$ is an environment and $\pi$ is an agent.
    Then
    \[
        V^{-\pi}_{-\mu}=-V^\pi_\mu
    \]
    (and the left-hand side is defined if and only if the right-hand side is defined).
\end{theorem}

\begin{proof}
    By Definition \ref{omnibusdefn} part 7,
    it suffices to show that for each $n\in\mathbb N$,
    $V^{-\pi}_{-\mu,n}=-V^\pi_{\mu,n}$.
    For that, it suffices to show that for every finite ORA-sequence
    $p$, if $p$ is empty or $p$ terminates with a reward or an action, then
    the probability $X$ of generating $p$ using $\pi$ and $\mu$
    (as in Definition \ref{omnibusdefn} part 6)
    equals the probability $X'$ of generating $-p$
    using $-\pi$ and $-\mu$. We will show this by induction on the length of $p$.

    Case 1: $p$ is empty. Then $X=X'=1$.

    Case 2: $p$ terminates with a reward.
    Then $p=q\frown \langle o,r\rangle$
    for some ORA-play $q$. Let $Y$ (resp.\ $Y'$) be the probability of generating $q$
    (resp.\ $-q$)
    using $\pi$ and $\mu$ (resp.\ $-\pi$ and $-\mu$). We reason:
    \begin{align*}
        X &= \mu(\langle o,r\rangle|q)Y
            &\mbox{(Definition of $X$)}\\
          &= \mu(\langle o,--r\rangle | {--q})Y
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
          &= (-\mu)(\langle o,-r\rangle | {-q})Y
            &\mbox{(Definition of $-\mu$)}\\
          &= (-\mu)(\langle o,-r\rangle | {-q})Y'
            &\mbox{(By induction, $Y=Y'$)}\\
          &= X'. &\mbox{(Definition of $X'$)}
    \end{align*}

    Case 3: $p$ terminates with an action.
    Then $p=q\frown a$ for some ORA-prompt $q$.
    Let $Y$ (resp.\ $Y'$) be the probability of generating $q$
    (resp.\ $-q$)
    using $\pi$ and $\mu$ (resp.\ $-\pi$ and $-\mu$). We reason:
    \begin{align*}
        X &= \pi(a|q)Y
            &\mbox{(Definition of $X$)}\\
          &= \pi(a|{--q})Y
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
          &= (-\pi)(a|{-q})Y
            &\mbox{(Definition of $-\pi$)}\\
          &= (-\pi)(a|{-q})Y'
            &\mbox{(By induction, $Y=Y'$)}\\
          &= X'. &\mbox{(Definition of $X'$)}
    \end{align*}
\end{proof}

\begin{corollary}
\label{twistcorollary}
    For every agent $\pi$ and environment $\mu$,
    \[V^\pi_{-\mu}=-V^{-\pi}_{\mu}\]
    (and the left-hand side is defined if and only if the right-hand side is defined).
\end{corollary}

\begin{proof}
    If neither side is defined, then there is nothing to prove.
    Assume the left-hand side is defined. Then
    \begin{align*}
        V^\pi_{-\mu} &= V^{--\pi}_{-\mu} &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
            &= -V^{-\pi}_\mu, &\mbox{(Theorem \ref{bigtheorem})}
    \end{align*}
    as desired. A similar argument holds if we assume the right-hand side is defined.
\end{proof}

\begin{corollary}
\label{wisminuswcorollary}
    For every environment $\mu$, $\mu$ is well-behaved if and only if $-\mu$
    is well-behaved.
\end{corollary}

\begin{proof}
    We prove the $\Rightarrow$ direction, the other is similar.
    Let $\pi$ be any agent. Since $\mu$ is well-behaved, $V^{-\pi}_\mu$ is defined,
    thus $V^\pi_{-\mu}=-V^{-\pi}_\mu$ is defined by Corollary \ref{twistcorollary}.
\end{proof}

\section{Symmetric Intelligence}

By definition, agent $-\pi$ acts exactly as agent $\pi$ would
act if $\pi$ were confused into thinking that punishments were rewards
and rewards were punishments.
Whatever ingenuity $\pi$ applies to maximize rewards,
$-\pi$ applies that exact same ingenuity to maximize punishments.
Thus, if $\Gamma$ is an intelligence function (intended to measure how
well an agent extracts rewards in some aggregate sense across the whole
infinite space of all well-behaved environments), then $\Gamma$ ought
to satisfy the equation:
\[
    \Gamma(-\pi) = -\Gamma(\pi).
\]
Not every intelligence function satisfies the above equation, but the
above equation should be considered as a \emph{desideratum} when we
invent ways of measuring intelligence: all else being equal, an
intelligence function which satisfies the above equation should be
considered better than an intelligence function which does not.
In this section, we will argue that the universal intelligence measure
proposed by Legg and Hutter can be improved in a natural way which
will make it satisfy the above equation.

\begin{definition}
(Complexity)
    Suppose $\mu$ is an environment.
    \begin{enumerate}
        \item By $K(\mu)$, we mean the Kolmogorov complexity of $\mu$.
        \item By $S(\mu)$, the \emph{symmetric complexity} of $\mu$,
        we mean
        \[
            S(\mu) = \max\{K(\mu), K(-\mu)\}.
        \]
    \end{enumerate}
\end{definition}

\begin{lemma}
\label{Sissymmetriclemma}
    For any environment $\mu$, $S(\mu) = S(-\mu)$.
\end{lemma}

\begin{proof}
    Trivial.
\end{proof}

\begin{definition}
    Let $W$ be the set of all well-behaved environments.
    Let $-W=\{-\mu\,:\,\mu\in W\}$.
\end{definition}

\begin{lemma}
\label{WequalsminusWlemma}
$W=-W$.
\end{lemma}

\begin{proof}
    By Corollary \ref{wisminuswcorollary}.
\end{proof}

\begin{definition}
\label{universalintelligencedefn}
(Universal Intelligence)
    Suppose $\pi$ is an agent.
    \begin{enumerate}
        \item
        The \emph{Legg-Hutter universal intelligence} $\Gamma_{\LH}(\pi)$ is
        \[
            \Gamma_{\LH}(\pi) = \sum_{\mu \in W} 2^{-K(\mu)}V^\pi_\mu.
        \]
        \item
        The \emph{symmetric universal intelligence} $\Gamma_{\SYM}(\pi)$ is
        \[
            \Gamma_{\SYM}(\pi) = \sum_{\mu\in W} 2^{-S(\mu)}V^\pi_\mu.
        \]
    \end{enumerate}
\end{definition}

The following theorem shows that $\Gamma_{\SYM}$ satisfies the above desideratum.

\begin{theorem}
\label{maintheorem}
    For every agent $\pi$,
    \[
        \Gamma_{\SYM}(-\pi) = -\Gamma_{\SYM}(\pi).
    \]
\end{theorem}

\begin{proof}
    Let $\pi$ be an agent. Then:
    \begin{align*}
        \Gamma_{\SYM}(-\pi) &= \sum_{\mu\in W} 2^{-S(\mu)}V^{-\pi}_\mu
            &\mbox{(Definition \ref{universalintelligencedefn})}\\
          &= -\sum_{\mu\in W} 2^{-S(\mu)}V^\pi_{-\mu}
            &\mbox{(Corollary \ref{twistcorollary})}\\
          &= -\sum_{\mu\in W} 2^{-S(-\mu)}V^\pi_{-\mu}
            &\mbox{(Lemma \ref{Sissymmetriclemma})}\\
          &= -\sum_{\mu\in -W} 2^{-S(\mu)}V^\pi_\mu
            &\mbox{(Change of variables)}\\
          &= -\sum_{\mu\in W} 2^{-S(\mu)}V^\pi_\mu
            &\mbox{(Lemma \ref{WequalsminusWlemma})}\\
          &= -\Gamma_{\SYM}(\pi).
            &\mbox{(Definition \ref{universalintelligencedefn})}
    \end{align*}
\end{proof}

The above desideratum, that $\Gamma(-\pi)=-\Gamma(\pi)$, applies to
numerical intelligence measures. If one is merely interested in binary
intelligence comparators (such as those in \cite{alexander2019intelligence}),
the desideratum can be weakened into a non-numerical comparator form:
\begin{quote}
    If $\pi$ is more intelligent than $\rho$,
    then $-\pi$ should be less intelligent than $-\rho$.
\end{quote}
The following corollary shows that $\Gamma_{\SYM}$ satisfies this desideratum too.

\begin{corollary}
    Let $\pi,\rho$ be agents. If $\Gamma_{\SYM}(\pi)>\Gamma_{\SYM}(\rho)$
    then $\Gamma_{\SYM}(-\pi)<\Gamma_{\SYM}(-\rho)$.
\end{corollary}

\begin{proof}
    By Theorem \ref{maintheorem} and basic algebra.
\end{proof}

The following corollary shows that $\Gamma_{\SYM}$ satisfies yet another obvious
desideratum.

\begin{corollary}
\label{ignoringrewardscorollary}
    Suppose $\pi$ is an agent which ignores rewards (by which we mean that
    $\pi(p)$ does not depend on the rewards in $p$).
    Then $\Gamma_{\SYM}(\pi)=0$.
\end{corollary}

\begin{proof}
    The hypothesis clearly implies $\pi=-\pi$. Thus by Theorem
    \ref{maintheorem}, $\Gamma_{\SYM}(\pi)=-\Gamma_{\SYM}(\pi)$,
    which forces $\Gamma_{\SYM}(\pi)=0$.
\end{proof}

Corollary \ref{ignoringrewardscorollary} illuminates why
symmetric complexity is an improvement over Kolmogorov complexity
in Definition \ref{universalintelligencedefn}.
Let $a\in\mathcal A$ be some default action and consider the
agent $\pi_a$ who always takes action $a$. For any particular environment $\mu$,
where $\pi_a$ earns some total reward $r$ by blind luck,
that total reward ought to be cancelled out by $-\mu$, where
the exact same blind luck becomes blind misfortune and $\pi_a$ earns total reward
$-r$. But if $K(\mu)\not=K(-\mu)$,
then the different weights $2^{-K(\mu)}\not=2^{-K(-\mu)}$ would prevent
these two outcomes from cancelling each other.


\bibliographystyle{plain}
\bibliography{sym}
\end{document}
