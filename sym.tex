\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\def\LH{\textrm{LH}}
\def\SYM{S}

\title{Symmetric Universal Intelligence: A Variation of Legg-Hutter Universal Intelligence}
\author{Samuel Allen Alexander}

\begin{document}

\maketitle

\begin{abstract}
    Can an agent's intelligence level be negative?
    We extend the Legg-Hutter agent-environment framework to include environments
    capable of punishing the agent with negative rewards. The Legg-Hutter
    intelligence of an agent attempts to measure how much reward the agent
    extracts from the universe of all environments in an aggregate sense. Thus,
    when the framework is extended to allow punishments, it naturally follows
    that an agent should have negative intelligence if that agent generally
    extracts more punishments than rewards.
    Extending the framework in this way, we discover algebraic structural properties
    not otherwise present. We introduce a variation of Legg-Hutter universal
    intelligence, which we call symmetric universal intelligence, and use said
    structural properties to show that this symmetric universal intelligence
    measure satisfies certain intuitive algebraic desiderata.
\end{abstract}

\section{Introduction}

In their brilliant paper \cite{legg2007universal}, Legg and Hutter write:
\begin{quote}
    ``As our goal is to produce a definition of intelligence that is as broad and
    encompassing as possible, the space of environments used in our definition should
    be as large as possible.''
\end{quote}
So motivated, we investigated what would happen if we extended the universe
of environments to include environments with rewards from $\mathbb Q\cap [-1,1]$
instead of just from $\mathbb Q\cap [0,1]$ as in Legg and Hutter's paper.
In other words, we investigated what would happen if environments are not only
allowed to reward agents but also to punish agents (a punishment being a negative
reward).

We discovered that when negative rewards are allowed, this
introduces a certain algebraic structure into the agent-environment framework. The
main objection we anticipate to our extended framework
is that it implies the negative intelligence of certain
agents\footnote{Thus, this paper falls under the broader
program of advocating for intelligence measures having different ranges than
the nonnegative reals. In other papers we have advocated
more extreme extensions of the range of intelligence measures
\cite{alexander2020archimedean}; by contrast, here we merely question the
assumption that intelligence never be negative, leaving aside the
question of whether intelligence should be real-valued.}.
We would argue that this makes perfect sense when environments are capable of punishing
agents: the intelligence of a reinforcement learning agent should measure the
degree to which that agent can extract large rewards on average across many environments;
an agent who instead extracts large punishments on average across many environments
should therefore have a negative intelligence level.

The structure of the paper is as follows:
\begin{itemize}
    \item In Section \ref{prelimsecn}, we give preliminary definitions.
    \item In Section \ref{dualsection}, we introduce what we call the dual of an agent and of
        an environment, and prove some algebraic theorems about these.
    \item In Section \ref{mainsecn}, we introduce symmetric universal intelligence, a variation
        of Legg-Hutter intelligence, and show it satisfies certain algebraic desiderata.
    \item In Section \ref{conclusionsecn}, we summarize and make concluding remarks, including
        remarks about how these ideas might be applied to certain other intelligence measures.
\end{itemize}

\section{Preliminaries}
\label{prelimsecn}

In defining agent and environment below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify which well-behaved
environments to restrict our attention to).

Throughout the paper, we implicitly
fix a finite set $\mathcal A$ of \emph{actions},
a finite set $\mathcal O$ of \emph{observations},
and a finite set $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards}
(so each reward is a rational number between $-1$ and $1$ inclusive),
with $|\mathcal A|>1$,
$|\mathcal O|>0$, $|\mathcal R|>0$.
We assume that $\mathcal R$ satisfies the following property:
whenever $\mathcal R$ contains any reward $r$, then $\mathcal R$
also contains $-r$.
We assume $\mathcal A$, $\mathcal O$, and $\mathcal R$ are mutually disjoint
(i.e., no reward is an action, no reward is an observation, and no action is an
observation).
The acronym ORA stands for ``Observation-Reward-Action''.
By $\langle\rangle$ we mean the empty sequence.

\begin{definition}
\label{omnibusdefn}
    (Agents, environments, etc.)
    \begin{enumerate}
        \item
        By an \emph{ORA-sequence}, we mean a sequence $s$ such that the following requirements
        hold for every natural number $n$:
        \begin{itemize}
            \item If $s$ has a $(3n)$th element, that element is an observation.
            \item If $s$ has a $(3n+1)$th element, that element is a reward.
            \item If $s$ has a $(3n+2)$th element, that element is an action.
        \end{itemize}
        Thus, informally speaking, an ORA-sequence is empty or
        looks like ``observation, reward, action, observation, reward, action, ...'';
        if it terminates (i.e., if it is finite), it may terminate with
        either an observation, a reward, or an action.
        \item
        By an \emph{ORA-prompt}, we mean a non-empty ORA-sequence which
        terminates with a reward.
        \item
        By an \emph{ORA-play}, we mean an ORA-sequence which is either empty or else
        terminates with an action.
        \item
        By an \emph{agent}, we mean a function $\pi$ which assigns to every
        ORA-prompt $p$ a probability measure $\pi(p)$ on $\mathcal A$.
        For each $a\in\mathcal A$,
        we write $\pi(a|p)$ for the probability assigned to $a$ by $\pi(p)$.
        Intuitively, we think of $\pi(a|p)$ as the probability that the agent $\pi$
        will take action $a$ in response to the ORA-prompt $p$.
        \item
        By an \emph{environment}, we mean a function $\mu$
        which assigns to every ORA-play $p$ a probability measure $\mu(p)$
        on $\mathcal O\times\mathcal R$.
        For each $o\in\mathcal O$ and $r\in\mathcal R$, we write $\mu(o,r|p)$
        for the probability assigned to $(o,r)$ by $\mu(p)$.
        Intuitively, we think of $\mu(o,r|p)$ as the
        probability that the environment
        $\mu$ will issue observation $o$ and reward $r$ to the agent in response
        to the ORA-play $p$.
        \item
        If $\pi$ is an agent, $\mu$ is an environment, and $n\in\mathbb N$,
        we write $V^\pi_{\mu,n}$ for the expected value of the sum of
        the first $n$ rewards which would occur in an ORA-sequence
        $(o_0,r_0,a_0,\ldots,o_n,r_n)$ randomly generated as follows:
        \begin{enumerate}
            \item $(o_0,r_0)\in \mathcal O\times\mathcal R$ is chosen randomly based
            on the probability measure $\mu(\langle\rangle)$.
            \item $a_0\in\mathcal A$ is chosen randomly based on the probability
            measure $\pi(o_0,r_0)$.
            \item
            For each $i>0$,
            $(o_i,r_i)\in\mathcal O\times\mathcal R$ is chosen randomly based on
            the probability measure $\mu(o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1})$.
            \item
            For each $i>0$,
            $a_i\in\mathcal A$ is chosen randomly based on the probability measure
            $\pi(o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1},o_i,r_i)$.
        \end{enumerate}
        \item
        If $\pi$ is an agent and $\mu$ is an environment,
        let $V^\pi_\mu=\lim_{n\to\infty}V^{\pi}_{\mu,n}$.
        Intuitively, $V^\pi_\mu$ is the expected total reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

Note that it is possible for $V^\pi_\mu$ to be undefined.
For example, if $\mu$ is an environment which always issues
reward $(-1)^n$ in response to the agent's $n$th action,
then $V^\pi_\mu$ is undefined for every agent $\pi$.
This would not be the case if rewards were required to be $\geq 0$,
so this is one way in which allowing
punishments complicates the resulting theory.

In order to define Legg-Hutter-style
intelligence measures, it is, in any case, necessary to restrict attention to
certain well-behaved environments. Legg and Hutter (in their Section 3.2)
restrict attention to environments $\mu$ that never give total reward more than $1$,
from which it follows that $V^\pi_\mu\leq 1$. This implication is less clear
when environments can punish the agent, so we take a different approach:
rather than limit the total reward the environment can give and use that to
force $V^\pi_\mu$ to be bounded, we will cut the middleman and
directly assume that $V^\pi_\mu$ is bounded.

\begin{definition}
    An environment $\mu$ is \emph{well-behaved} if $\mu$ is computable and the following
    condition holds: for every agent $\pi$, $V^\pi_\mu$ exists and
    $-1\leq V^\pi_\mu\leq 1$.
\end{definition}

\section{Dual Agents and Dual Environments}
\label{dualsection}

In the Introduction, we promised that by allowing environments to punish agents,
we would reveal algebraic structure not otherwise present. The key to this additional
structure is the following definition.

\begin{definition}
(Dual Agents and Dual Environments)
\begin{enumerate}
    \item
    Suppose $s$ is an ORA-sequence. Let $-s$
    be the ORA-sequence which is equal to $s$ in every way except that,
    whenever the $(3n+1)$th element of $s$ is a reward $r$,
    then the $(3n+1)$th element of $-s$ is $-r$.
    \item
    Suppose $\pi$ is an agent.
    We define a new agent $-\pi$, the \emph{dual} of $\pi$,
    as follows:
    for each ORA-prompt $p$, for each action $a\in\mathcal A$,
    \[(-\pi)(a|p)=\pi(a|{-p}).\]
    \item
    Suppose $\mu$ is an environment.
    We define a new environment $-\mu$, the \emph{dual} of $\mu$,
    as follows:
    for each ORA-play $p$, for each observation $o\in\mathcal O$
    and reward $r\in\mathcal R$,
    \[(-\mu)(o,r|p)=\mu(o,-r|{-p}).\]
\end{enumerate}
\end{definition}

\begin{lemma}
\label{doublesubtractionlemma}
(Double Negation)
    \begin{enumerate}
        \item For each ORA-sequence $s$, $--s=s$.
        \item For each agent $\pi$, $--\pi=\pi$.
        \item For each environment $\mu$, $--\mu=\mu$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    Follows from the fact that for every real number $r$, $--r=r$.
\end{proof}

In the proof of the following theorem, we write $\frown$ for concatenation.

\begin{theorem}
\label{bigtheorem}
    Suppose $\mu$ is an environment and $\pi$ is an agent.
    Then
    \[
        V^{-\pi}_{-\mu}=-V^\pi_\mu
    \]
    (and the left-hand side is defined if and only if the right-hand side is defined).
\end{theorem}

\begin{proof}
    By Definition \ref{omnibusdefn} part 7,
    it suffices to show that for each $n\in\mathbb N$,
    $V^{-\pi}_{-\mu,n}=-V^\pi_{\mu,n}$.
    For that, it suffices to show that for every finite ORA-sequence
    $p$, if $p$ is empty or $p$ terminates with a reward or an action, then
    the probability $X$ of generating $p$ using $\pi$ and $\mu$
    (as in Definition \ref{omnibusdefn} part 6)
    equals the probability $X'$ of generating $-p$
    using $-\pi$ and $-\mu$. We will show this by induction on the length of $p$.

    Case 1: $p$ is empty. Then $X=X'=1$.

    Case 2: $p$ terminates with a reward.
    Then $p=q\frown o \frown r$
    for some ORA-sequence $q$ which terminates with an action.
    Let $Y$ (resp.\ $Y'$) be the probability of generating $q$
    (resp.\ $-q$)
    using $\pi$ and $\mu$ (resp.\ $-\pi$ and $-\mu$). We reason:
    \begin{align*}
        X &= \mu(o,r|q)Y
            &\mbox{(Definition of $X$)}\\
          &= \mu(o,--r| {--q})Y
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
          &= (-\mu)(o,-r| {-q})Y
            &\mbox{(Definition of $-\mu$)}\\
          &= (-\mu)(o,-r| {-q})Y'
            &\mbox{(By induction, $Y=Y'$)}\\
          &= X'. &\mbox{(Definition of $X'$)}
    \end{align*}

    Case 3: $p$ terminates with an action.
    Then $p=q\frown a$ for some ORA-sequence $q$ which terminates with a reward.
    Let $Y$ (resp.\ $Y'$) be the probability of generating $q$
    (resp.\ $-q$)
    using $\pi$ and $\mu$ (resp.\ $-\pi$ and $-\mu$). We reason:
    \begin{align*}
        X &= \pi(a|q)Y
            &\mbox{(Definition of $X$)}\\
          &= \pi(a|{--q})Y
            &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
          &= (-\pi)(a|{-q})Y
            &\mbox{(Definition of $-\pi$)}\\
          &= (-\pi)(a|{-q})Y'
            &\mbox{(By induction, $Y=Y'$)}\\
          &= X'. &\mbox{(Definition of $X'$)}
    \end{align*}
\end{proof}

\begin{corollary}
\label{twistcorollary}
    For every agent $\pi$ and environment $\mu$,
    \[V^\pi_{-\mu}=-V^{-\pi}_{\mu}\]
    (and the left-hand side is defined if and only if the right-hand side is defined).
\end{corollary}

\begin{proof}
    If neither side is defined, then there is nothing to prove.
    Assume the left-hand side is defined. Then
    \begin{align*}
        V^\pi_{-\mu} &= V^{--\pi}_{-\mu} &\mbox{(Lemma \ref{doublesubtractionlemma})}\\
            &= -V^{-\pi}_\mu, &\mbox{(Theorem \ref{bigtheorem})}
    \end{align*}
    as desired. A similar argument holds if we assume the right-hand side is defined.
\end{proof}

\begin{corollary}
\label{wisminuswcorollary}
    For every environment $\mu$, $\mu$ is well-behaved if and only if $-\mu$
    is well-behaved.
\end{corollary}

\begin{proof}
    We prove the $\Rightarrow$ direction, the other is similar.
    Since $\mu$ is well-behaved, $\mu$ is computable, so clearly $-\mu$ is computable.
    Let $\pi$ be any agent. Since $\mu$ is well-behaved, $V^{-\pi}_\mu$ is defined
    and $-1\leq V^{-\pi}_\mu\leq 1$.
    By Corollary \ref{twistcorollary},
    $V^\pi_{-\mu}=-V^{-\pi}_\mu$ is defined,
    implying $-1\leq V^\pi_{-\mu}\leq 1$.
    By arbitrariness of $\pi$, this shows $-\mu$ is well-behaved.
\end{proof}

\section{Symmetric Intelligence}
\label{mainsecn}

By definition, agent $-\pi$ acts exactly as agent $\pi$ would
act if $\pi$ were confused into thinking that punishments were rewards
and rewards were punishments.
Whatever ingenuity $\pi$ applies to maximize rewards,
$-\pi$ applies that exact same ingenuity to maximize punishments.
Thus, if $\Gamma$ is an intelligence function (intended to measure how
well an agent extracts rewards in some aggregate sense across the whole
infinite space of all well-behaved environments), then $\Gamma$ ought
to satisfy the equation:
\[
    \Gamma(-\pi) = -\Gamma(\pi).
\]
Not every intelligence function satisfies the above equation, but the
above equation should be considered desirable when we
invent ways of measuring intelligence: all else being equal, an
intelligence function which satisfies the above equation should be
considered better than an intelligence function which does
not\footnote{This depends strongly on our assumption that when we
measure intelligence we are measuring aggregate reward-extraction. This might clash
with everyday ideas of intelligence: if a mischievous child answers every single
question wrong on a true-false IQ test, we would say that child is clearly very
intelligent, and is using said intelligence to deliberately fail the test.
To quote Socrates, ``Don't you think the ignorant person would often involuntarily
tell the truth when he wished to say falsehoods, if it so happened, because he
didn't know; whereas you, the wise person, if you should wish to lie,
would always consistently lie?''\ \cite{lesserhippias}
Likewise if an agent consistently manages to extract the worst possible rewards
from many environments, we might think the agent is brilliant but
masochistic.
Nevertheless, we would not want to put said agent
in charge of navigating environments for us (at least not without some
reverse psychology).}.
In this section, we will show that the universal intelligence measure
proposed by Legg and Hutter satisfies the above equation, provided the
backward model of computation is suitably chosen.

\begin{definition}
    (Prefix-free universal Turing machines)
    \begin{enumerate}
        \item A partial computable function $f:\subseteq 2^*\to 2^*$
        is \emph{prefix-free} if the following requirement holds:
        for all $p,p'\in 2^*$, if $p$ is a strict substring of $p'$,
        then $p$ and $p'$ are not both in the domain of $f$.
        \item A \emph{prefix-free universal Turing machine}
        (or \emph{PFUTM}) is a prefix-free
        partial computable function $U:\subseteq 2^*\to 2^*$
        such that the following condition holds.
        For every prefix-free partial computable function
        $f:\subseteq 2^*\to 2^*$, $\exists p\in 2^*$ such that
        $\forall x\in 2^*$, $f(x)=U(p\frown x)$.
    \end{enumerate}
\end{definition}

Fix a computable prefix-free binary encoding of
$(\mathcal O\times \mathcal R\times\mathcal A)^*$, in other words,
fix a computable function
$\ulcorner\bullet\urcorner:(\mathcal O\times \mathcal R\times\mathcal A)^*\to 2^*$
assigning a distinct code $\ulcorner p\urcorner\in 2^*$ to every sequence
$p\in (\mathcal O\times \mathcal R\times\mathcal A)^*$ in such a way that
$\ulcorner p\urcorner$ is never a substring of $\ulcorner p'\urcorner$
for any distinct $p,p'\in (\mathcal O\times \mathcal R\times\mathcal A)^*$.
That this can be done is clear by the Church-Turing thesis.
Likewise, fix prefix-free codes $\ulcorner (o,r)\urcorner$ for all
pairs $(o,r)\in\mathcal O\times\mathcal R$.

\begin{definition}
(Kolmogorov Complexity)
Suppose $U$ is any PFUTM.
\begin{enumerate}
    \item
    For any prefix-free partial computable function $f: \subseteq 2^*\to 2^*$, the
    \emph{Kolmogorov complexity of $f$ according to $U$},
    written $K_U(f)$, is the smallest $n\in\mathbb N$ such that
    there is some $p\in 2^*$ with length $n$ such that
    $\forall x\in 2^*$, $f(x)=U(p\frown x)$.
    \item
    Suppose $\mu$ is a computable environment.
    We define $K_U(\mu)$ to be $K_U(f)$ where $f$
    is the prefix-free partial computable function defined by
    $f(\ulcorner p\urcorner)=\ulcorner \mu(p)\urcorner$
    for every $p\in (\mathcal O\times \mathcal R\times\mathcal A)^*$.
    \item
    We say a PFUTM $U$ is \emph{enviro-symmetric} if
    $K_U(\mu)=K_U(-\mu)$ for every environment $\mu$.
\end{enumerate}
\end{definition}

The following lemma shows that enviro-symmetric PFUTMs exist; in fact, any PFUTM
can be modified into one in a simple way.

\begin{lemma}
\label{envirosymmexistencelemma}
    Given a PFUTM $U_0$, define a new PFUTM $U$ as follows ($x\in 2^*$).
    \begin{enumerate}
        \item If $x=0\frown x_0$, let $U(x)=U_0(x_0)$.
        \item If $x=1\frown x_0$, systematically enumerate all
        $p\in (\mathcal O\times \mathcal R\times\mathcal A)^*$
        looking for some $p$ such that $x_0=y\frown \ulcorner p\urcorner$.
        If no such $p$ exist, $U(x)$ is undefined.
        Otherwise, $p$ is unique by prefix-freeness of $\ulcorner\bullet\urcorner$.
        If $U_0(y\frown\ulcorner -p\urcorner)=\ulcorner (o,r)\urcorner$
        for some $(o,r)\in\mathcal O\times\mathcal R$ then
        let $U(x)=\ulcorner (o,-r)\urcorner$. Otherwise, $U(x)$ is undefined.
    \end{enumerate}
    Then $U$ is enviro-symmetric.
\end{lemma}

\begin{proof}
    That $U$ is a PFUTM is clear.
    To show that $U$ is enviro-symmetric, let $\mu$ be an environment.
    Let $q\in 2^*$ be minimal-length such that
    $\forall p\in (\mathcal O\times \mathcal R\times\mathcal A)^*$,
    $U(q\frown \ulcorner p\urcorner)=\ulcorner \mu(p)\urcorner$
    (loosely speaking, such that $U(q\frown\bullet)$ computes $\mu$).
    If $q=0\frown q_0$ then by construction $U$ is designed so that $q'=1\frown q_0$
    has the property that $U(q'\frown \bullet)$ computes $-\mu$.
    On the other hand, if $q=1\frown q_0$ then the fact that $U(q\frown\bullet)$ computes
    $--\mu$ implies (by how $U(1\frown q_0\frown\bullet)$ is defined) that
    $q'=0\frown q_0$ has the property that $U(q'\frown\bullet)$ computes $-\mu$.
    Either way, $K_U(-\mu)\leq K_U(\mu)$. By a symmetric argument,
    $K_U(\mu)\leq K_U(-\mu)$, so $K_U(\mu)=K_U(-\mu)$.
\end{proof}

\begin{definition}
    Let $W$ be the set of all well-behaved environments.
    Let $-W=\{-\mu\,:\,\mu\in W\}$.
\end{definition}

\begin{lemma}
\label{WequalsminusWlemma}
$W=-W$.
\end{lemma}

\begin{proof}
    By Corollary \ref{wisminuswcorollary}.
\end{proof}

\begin{definition}
\label{universalintelligencedefn}
For every agent $\pi$
and PFUTM $U$, the \emph{Legg-Hutter universal intelligence of $\pi$ given
by $U$}, written $\LH_U(\pi)$, is
\[
    \LH_U(\pi) = \sum_{\mu \in W} 2^{-K_U(\mu)}V^\pi_\mu.
\]
\end{definition}

The sum defining $\LH_U(\pi)$ is absolutely convergent because the summands
in absolute value are dominated by the summands defining Chaitin's constant
(this is why we insist on the UTM being prefix-free).
Thus, by a well-known
theorem from elementary calculus, the sum does not depend on which order the $\mu\in W$
are enumerated.

The following theorem shows that for every enviro-symmetric PFUTM $U$, $\LH_U$
satisfies the above desideratum.

\begin{theorem}
\label{maintheorem}
    For every agent $\pi$ and enviro-symmetric PFUTM $U$,
    \[
        \LH_U(-\pi) = -\LH_U(\pi).
    \]
\end{theorem}

\begin{proof}
    Let $\pi$ be an agent. Then:
    \begin{align*}
        \LH_U(-\pi) &= \sum_{\mu\in W} 2^{-K_U(\mu)}V^{-\pi}_\mu
            &\mbox{(Definition \ref{universalintelligencedefn})}\\
          &= -\sum_{\mu\in W} 2^{-K_U(\mu)}V^\pi_{-\mu}
            &\mbox{(Corollary \ref{twistcorollary})}\\
          &= -\sum_{\mu\in W} 2^{-K_U(-\mu)}V^\pi_{-\mu}
            &\mbox{($U$ is enviro-symmetric)}\\
          &= -\sum_{\mu\in -W} 2^{-K_U(\mu)}V^\pi_\mu
            &\mbox{(Change of variables)}\\
          &= -\sum_{\mu\in W} 2^{-K_U(\mu)}V^\pi_\mu
            &\mbox{(Lemma \ref{WequalsminusWlemma})}\\
          &= -\LH_{U'}(\pi).
            &\mbox{(Definition \ref{universalintelligencedefn})}
    \end{align*}
\end{proof}

% The above desideratum, that $\Gamma(-\pi)=-\Gamma(\pi)$, applies to
% numerical intelligence measures. If one is merely interested in binary
% intelligence comparators (such as those in \cite{alexander2019intelligence}),
% the desideratum can be weakened into a non-numerical comparator form:
% \begin{quote}
%     If $\pi$ is more intelligent than $\rho$,
%     then $-\pi$ should be less intelligent than $-\rho$.
% \end{quote}
% The following corollary shows that $\Gamma_{\SYM}$ satisfies this desideratum too.

% \begin{corollary}
% \label{comparatorcorollary}
%     Let $\pi,\rho$ be agents. If $\Gamma_{\SYM}(\pi)>\Gamma_{\SYM}(\rho)$
%     then $\Gamma_{\SYM}(-\pi)<\Gamma_{\SYM}(-\rho)$.
% \end{corollary}

% \begin{proof}
%     By Theorem \ref{maintheorem} and basic algebra.
% \end{proof}

The following corollary shows that with suitable choice of UTM,
Legg-Hutter universal intelligence satisfies another obvious desideratum.

\begin{corollary}
\label{ignoringrewardscorollary}
    Let $U$ be an enviro-symmetric PFUTM and
    suppose $\pi$ is an agent which ignores rewards (by which we mean that
    $\pi(p)$ does not depend on the rewards in $p$).
    Then $\LH_U(\pi)=0$.
\end{corollary}

\begin{proof}
    The hypothesis clearly implies $\pi=-\pi$. Thus by Theorem
    \ref{maintheorem}, $\LH_U(\pi)=-\LH_U(\pi)$,
    forcing $\LH_U(\pi)=0$.
\end{proof}

Corollary \ref{ignoringrewardscorollary} illustrates why it is appropriate, for
purposes of Legg-Hutter universal intelligence, to choose an enviro-symmetric PFUTM.
Consider an agent $\pi_a$
which always blindly repeats a fixed action $a\in\mathcal A$.
For any particular environment $\mu$,
where $\pi_a$ earns some total reward $r$ by blind luck,
that total reward ought to be cancelled out by $-\mu$, where
the exact same blind luck becomes blind misfortune and $\pi_a$ earns total reward
$-r$ (Corollary \ref{twistcorollary}). But if $K_U(\mu)\not=K_U(-\mu)$,
then the different weights $2^{-K_U(\mu)}\not=2^{-K_U(-\mu)}$ would prevent
these two outcomes from cancelling each other.

The above paragraph suggests that in the context of Legg-Hutter universal intelligence,
enviro-symmetricity is one answer to Leike and Hutter's question \cite{leike2015bad},
``But what are other desirable properties of a UTM?'' We would argue it also sheds light
on the more general question of what makes a UTM ``natural''. Enviro-symmetricity is
natural for the context of Legg-Hutter universal intelligence but would be utterly
irrelevant in contexts not having to do with reinforcement learning environments---just
as, if we added first-class web-programming support to, say, the C programming language,
that would be appropriate for web-programming, but for other applications it would just
clutter up the language. We opine that in the same way, this might explain the failure of
attempts \cite{muller2010stationary} to find UTMs that are ``natural'' across-the-board,
outside of specific contexts.


% Similarly, if an environment
% $\mu$ ignores agent actions, Corollary \ref{twistcorollary} implies
% the contributions of $\mu$ and $-\mu$ cancel each other out in $\Gamma_{\SYM}(\pi)$,
% which is desirable because why should an agent's intelligence depend on
% its interaction with an environment that ignores it?

% Of course, Kolmogorov complexity and hence Legg-Hutter intelligence depends
% implicitly on the background model of computation \cite{leike2015bad}.
% One could contrive a background model of computation where $K=S$,
% in which case the distinction in
% Definition \ref{universalintelligencedefn} would disappear.

\section{Conclusion}
\label{conclusionsecn}

By extending our attention to environments which can punish agents,
we became aware of algebraic structure in the agent-environment
framework (Section \ref{dualsection}). This structure allowed us to
show that by varying the weights used in the Legg-Hutter universal
intelligence construction, we can construct a similar intelligence
measure which satisfies key desiderata (Theorem \ref{maintheorem},
and Corollary \ref{ignoringrewardscorollary}).

Similar modifications could be made to other intelligence
measures based on the original Legg-Hutter idea, such as the anytime
intelligence measure of Hern{\'a}ndez-Orallo and Dowe \cite{hernandez}
and perhaps the realtime intelligence measure of Gavane \cite{gavane},
although the latter may involve greater care since negating numbers
takes a nonzero amount of time. The precise strategy we employ in this
paper is not relevant to Hibbardian intelligence measures of agents
who compete in adversarial sequence prediction games
\cite{hibbard} \cite{alexander2021measuring}. However, the higher-level
idea still applies: a predictor who always dumbly
makes the same prediction should have intelligence $0$, and yet, in some sense
such a predictor still outperforms predictors who intentionally try
to lose, and thus one could argue that intentionally-losing predictors
really ought to have negative Hibbardian intelligence.

Beyond the technical details, at a higher level, a key takeaway of
this paper is that if intelligence is supposed to measure how much
reward an agent extracts in aggregate across many environments,
including environments capable of punishing the agent, then such
intelligence measures ought to be permitted to output negative
values for certain agents. It is fun to imagine the trouble
that maximally anti-intelligent
agents, such as the negative version of the universally intelligent
agent AIXI \cite{hutter2004universal}, would get themselves into.


\bibliographystyle{plain}
\bibliography{sym}
\end{document}
