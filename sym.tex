\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}

\title{Symmetric Intelligence: An Improvement to Legg-Hutter Universal Intelligence}
\author{Samuel Allen Alexander}

\begin{document}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In their well-known paper \cite{legg2007universal}, Legg and Hutter write:
\begin{quote}
    ``As our goal is to produce a definition of intelligence that is as broad and
    encompassing as possible, the space of environments used in our definition should
    be as large as possible.''
\end{quote}
So motivated, we investigated what would happen if we extended the universe
of environments to include environments with rewards from $\mathbb Q\cap [-1,1]$
instead of just from $\mathbb Q\cap [0,1]$ as in Legg and Hutter's paper.
In other words, we investigated what would happen if environments are not only
allowed to reward agents but also to punish agents (a punishment being a negative
reward).

We discovered that when negative rewards are allowed, this
introduces a certain algebraic structure into the agent-environment framework, which
in turn allows an intelligence definition that is both better-behaved and also
easier to compute. The main objection we anticipate to our proposed improved
intelligence measure is that it assigns negative intelligence to certain agents.
We will argue that this makes sense when environments are capable of punishing
agents: the intelligence of a reinforcement learning agent should measure the
degree to which that agent can extract large rewards on average across many environments;
an agent who instead extracts large punishments on average across many environments
should therefore have a negative intelligence level.

\section{Preliminaries}

In defining agent and environment below, we attempt to follow
Legg and Hutter \cite{legg2007universal} as closely as possible,
except that we permit environments to output rewards from $\mathbb Q \cap [-1,1]$
rather than just $\mathbb Q\cap [0,1]$ (and, accordingly, we modify what it means
for an environment to be bounded).

Throughout the paper, we implicitly
fix a finite set $\mathcal A$ of \emph{actions},
a finite set $\mathcal O$ of \emph{observations},
and a finite set $\mathcal R\subseteq \mathbb Q\cap [-1,1]$ of \emph{rewards}
(so each reward is a rational number between $-1$ and $1$ inclusive),
with $|\mathcal A|>1$,
$|\mathcal O|>0$, $|\mathcal R|>0$.
We assume $\mathcal A$, $\mathcal O$, and $\mathcal R$ are mutually disjoint
(i.e., no reward is an action, no reward is an observation, and no action is an
observation).
The acronym ORA stands for ``observation-reward-action''.
Throughout the paper, $\langle\rangle$ denotes the empty sequence.

\begin{definition}
    (Agents and environments)
    \begin{enumerate}
        \item
        By an \emph{ORA-sequence}, we mean a sequence $s$ such that the following requirements
        hold for every natural number $n$:
        \begin{itemize}
            \item If $s$ has a $(3n)$th element, that element is an observation.
            \item If $s$ has a $(3n+1)$th element, that element is a reward.
            \item If $s$ has a $(3n+2)$th element, that element is an action.
        \end{itemize}
        Thus, informally speaking, a non-empty ORA-sequence
        looks like ``observation, reward, action, observation, reward, action, ...'';
        if it terminates (i.e., if it is finite), it may terminate with
        either an observation, a reward, or an action.
        \item
        By an \emph{ORA-prompt}, we mean a non-empty ORA-sequence which
        terminates with a reward.
        \item
        By an \emph{ORA-play}, we mean an ORA-sequence which is either empty or else
        terminates with an action.
        \item
        By an \emph{agent}, we mean a function $\pi$ which assigns to every
        ORA-prompt $p$ a probability measure $\pi(p)$ on $\mathcal A$.
        For each $a\in\mathcal A$,
        we write $\pi(a|p)$ for the probability assigned to $a$ by $\pi(p)$.
        Intuitively, we think of $\pi(a|p)$ as the probability that the agent $\pi$
        will take action $a$ in response to the ORA-prompt $p$.
        \item
        By an \emph{environment}, we mean a function $\mu$
        which assigns to every ORA-play $p$ a probability measure $\mu(p)$
        on $\mathcal O\times\mathcal R$.
        For each $o\in\mathcal O$ and $r\in\mathcal R$, we write $\mu(or|p)$
        for the probability assigned to $(o,r)$ by $\mu(p)$.
        Intuitively, we think of $\mu(or|p)$ as the probability that the environment
        $\mu$ will issue observation $o$ and reward $r$ to the agent in response
        to the ORA-play $p$.
        \item
        An infinite ORA-sequence $(o_0,r_0,a_0,\ldots)$ is \emph{possible for agent
        $\pi$ interacting with environment $\mu$} if the following requirements hold.
        \begin{itemize}
            \item The probability $\mu(o_0r_0|\langle\rangle)$ is nonzero.
            \item The probability $\pi(a_0|o_0r_0)$ is nonzero.
            \item For each $i>0$, the probability
                $\mu(o_ir_i|o_0r_0a_0\cdots o_{i-1}r_{i-1}a_{i-1})$ is nonzero.
            \item For each $i>0$, the probability
                $\pi(a_i|o_0r_0a_0\cdots o_{i-1}r_{i-1}a_{i-1}o_ir_i)$ is nonzero.
        \end{itemize}
        \item
        An environment $\mu$ is \emph{bounded} if for every agent $\pi$,
        for every infinite ORA-sequence $s=(o_0,r_0,a_0,\ldots)$,
        if $s$ is possible for $\pi$ interacting with $\mu$, then
        $\sum_{i=0}^\infty r_i$ converges and $-1\leq \sum_{i=0}^\infty r_i\leq 1$.
        \item
        If $\pi$ is an agent and $\mu$ is a bounded environment,
        we write $V^\pi_\mu$ for the expected value of the sum of the rewards in an
        ORA-sequence $(o_0,r_0,a_0,o_1,r_1,a_1,\ldots)$ randomly obtained as follows:
        \begin{enumerate}
            \item $(o_0,r_0)\in \mathcal O\times\mathcal R$ is chosen randomly based
            on the probability measure $\mu(\langle\rangle)$.
            \item $a_0\in\mathcal A$ is chosen randomly based on the probability
            measure $\pi(o_0,r_0)$.
            \item
            For each $i>0$,
            $(o_i,r_i)\in\mathcal O\times\mathcal R$ is chosen randomly based on
            the probability measure $\mu(o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1})$.
            \item
            For each $i>0$,
            $a_i\in\mathcal A$ is chosen randomly based on the probability measure
            $\pi(o_0,r_0,a_0,\ldots,o_{i-1},r_{i-1},a_{i-1},o_i,r_i)$.
        \end{enumerate}
        Intuitively, $V^\pi_\mu$ is the expected reward which $\pi$ would extract
        from $\mu$.
    \end{enumerate}
\end{definition}

\bibliographystyle{plain}
\bibliography{sym}
\end{document}
